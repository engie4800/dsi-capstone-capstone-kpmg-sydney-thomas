{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd     \n",
    "import warnings\n",
    "from langchain.chains import GraphCypherQAChain\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Dict, List\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from prompts_3layer import LAYER_ONE_PROMPT, LAYER_TWO_PROMPT, LAYER_THREE_PROMPT, QA_PROMPT\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from prompts_3layer import node_properties_query, rel_properties_query, rel_query, schema_text\n",
    "from tqdm import tqdm\n",
    "from kpmg_chatbot_v6 import Chat\n",
    "\n",
    "API_KEY = \"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\"\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding.bge_m3 import BGEM3FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPT:\n",
    "    \n",
    "    def __init__(self, graphDB: Neo4jGraph):\n",
    "        self.graphDB = graphDB\n",
    "        self.Tool1 = ThreeLayerGPT(self.graphDB)\n",
    "        self.Tool2 = BaseGPT(self.graphDB)\n",
    "        self.Tool3 = Chat(\"neo4j+s://bed4a8a0.databases.neo4j.io\", \"neo4j\", \"XbfIA4FEeKePbNnhGT2YtqQ8zqPCcSR7XYYMQbbb70I\")\n",
    "        self.model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "        \n",
    "    def __call__(self, file, export=True):\n",
    "        self._loadTestSet(file)\n",
    "        self._getAnswer()\n",
    "        self._getSimilarity(\"Intermediate GPT Answer\")\n",
    "        self._getSimilarity(\"Base GPT Answer\")\n",
    "        self._getSimilarity(\"Final GPT Answer\")\n",
    "        if export:\n",
    "            self.testSet.to_csv(\"Final Test Result(Error Cypher Shown)_v2.csv\", index=False)\n",
    "        \n",
    "    def _loadTestSet(self, file):\n",
    "        self.testSet = pd.read_csv(file)\n",
    "        \n",
    "    def _getAnswer(self):\n",
    "        self.testSet[\"Intermediate GPT Answer\"] = \"\"\n",
    "        self.testSet[\"Base GPT Answer\"] = \"\"\n",
    "        self.testSet[\"Final GPT Answer\"] = \"\"\n",
    "        \n",
    "        self.testSet[\"Intermediate GPT Cypher\"] = \"\"\n",
    "        self.testSet[\"Final GPT Cypher\"] = \"\"\n",
    "        \n",
    "        for i in tqdm(range(self.testSet.shape[0])):\n",
    "            message = self.testSet[\"Prompts\"][i]\n",
    "            self.testSet[\"Intermediate GPT Answer\"][i], query_mid = self.Tool1(message)\n",
    "            if self.testSet[\"Intermediate GPT Answer\"][i] == \"Error Query\":\n",
    "                self.testSet[\"Intermediate GPT Cypher\"][i] = query_mid\n",
    "            self.testSet[\"Base GPT Answer\"][i], query_base = self.Tool2(message)\n",
    "            self.testSet[\"Final GPT Answer\"][i], query_final = self.Tool3(message)\n",
    "            if self.testSet[\"Final GPT Answer\"][i] == \"Error Query\":\n",
    "                self.testSet[\"Final GPT Cypher\"][i] = query_final\n",
    "    \n",
    "    def _getSimilarity(self, col):\n",
    "        col_name = col + \" Score\"\n",
    "        self.testSet[col_name] = \"None\"\n",
    "        for i in range(self.testSet.shape[0]):\n",
    "            true_answer = [self.testSet[\"True Answer\"][i]]\n",
    "            generated_answer = [self.testSet[col][i]]\n",
    "            sentence_pairs = [[m, n] for m in true_answer for n in generated_answer]\n",
    "            res = self.model.compute_score(sentence_pairs,\n",
    "                    max_passage_length=128,\n",
    "                    weights_for_different_modes=[0.4, 0.2, 0.4])  \n",
    "            self.testSet[col_name][i] = res['colbert+sparse+dense'][0]\n",
    "            \n",
    "class ThreeLayerGPT:\n",
    "    \n",
    "    def __init__(self, graphDB: Neo4jGraph):\n",
    "        self.prompts = [LAYER_ONE_PROMPT, LAYER_TWO_PROMPT, LAYER_THREE_PROMPT]\n",
    "        self.chat1 = ChatOpenAI(api_key=API_KEY, temperature=0)\n",
    "        self.chat2 = ChatOpenAI(api_key=API_KEY, temperature=0)\n",
    "        self.chat3 = ChatOpenAI(api_key=API_KEY, temperature=0)\n",
    "        self.chat4 = ChatOpenAI(api_key=API_KEY, temperature=0)\n",
    "        self.graphDB = graphDB\n",
    "        \n",
    "    def __call__(self, message_input):\n",
    "        return self.getResult(message_input)\n",
    "    \n",
    "    def _getSchema(self):\n",
    "        node_props = self.graphDB.query(node_properties_query)\n",
    "        rel_props = self.graphDB.query(rel_properties_query)\n",
    "        rels = self.graphDB.query(rel_query)\n",
    "        node_props = [value for d in node_props for value in d.values()]\n",
    "        rel_props = [value for d in rel_props for value in d.values()]\n",
    "        rels = [value for d in rels for value in d.values()]\n",
    "        schema = schema_text(node_props, rel_props, rels)\n",
    "        return schema\n",
    "        \n",
    "    def _distillation(self, message):\n",
    "        messages = [\n",
    "        SystemMessage(\n",
    "            content=self.prompts[0]\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=message\n",
    "        )]\n",
    "        response = self.chat1.invoke(messages)\n",
    "        content = response.dict()['content']\n",
    "        return content        \n",
    "        \n",
    "    def _queryLogic(self, message):\n",
    "        messages = [\n",
    "        SystemMessage(\n",
    "            content=self.prompts[1]\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=message\n",
    "        )]\n",
    "        response = self.chat2.invoke(messages)\n",
    "        content = response.dict()['content']\n",
    "        return content  \n",
    "    \n",
    "    def _queryGeneration(self, message):\n",
    "        messages = [\n",
    "        SystemMessage(\n",
    "            content=self.prompts[2]\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=message\n",
    "        )]\n",
    "        response = self.chat3.invoke(messages)\n",
    "        content = response.dict()['content']\n",
    "        return content  \n",
    "    \n",
    "    def _generateAnswer(self, message):\n",
    "        human_prompt_template = PromptTemplate.from_template(\n",
    "        template= \"The original question is {question}, the previous LLM has generate intermediate result {answer}\")\n",
    "\n",
    "        human_prompt = human_prompt_template.format(\n",
    "            question =  message[\"question\"], \n",
    "            answer = message[\"answer\"]\n",
    "        )\n",
    "        \n",
    "        PROMPT = QA_PROMPT + \"\\n\" + self._getSchema()\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=PROMPT\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content= human_prompt\n",
    "            )]\n",
    "        \n",
    "        response = self.chat4.invoke(messages)\n",
    "        content = response.dict()['content']\n",
    "        return content           \n",
    "    \n",
    "    def getResult(self, message_input):\n",
    "        response = self._distillation(message_input)\n",
    "        message_layer1 = \"User Requirement:\" + str(message_input) + \"\\n\" + \"Query Logic and Relationship:\" + str(response)\n",
    "        response = self._queryLogic(message_layer1)\n",
    "        updatemessage = \"User Requirement:\" + str(message_input) + \"\\n\" + \"Query Logic Extraction:\" + str(response)\n",
    "        query = self._queryGeneration(updatemessage) \n",
    "        try:\n",
    "            data = self.graphDB.query(query)  \n",
    "        except:\n",
    "            return \"Error Query\", query\n",
    "        user_message = {\"question\": message_input, \"answer\": data}\n",
    "        final_output = self._generateAnswer(user_message)\n",
    "        return final_output, query\n",
    "\n",
    "class BaseGPT:\n",
    "    \n",
    "    def __init__(self, graphDB: Neo4jGraph):\n",
    "        self.graphDB = graphDB\n",
    "        self.chain = GraphCypherQAChain.from_llm(\n",
    "            llm = ChatOpenAI(temperature=0, api_key=API_KEY),\n",
    "            graph=self.graphDB, verbose=False\n",
    "        )\n",
    "    \n",
    "    def __call__(self, message):\n",
    "        query = None\n",
    "        try:\n",
    "            res = self.chain(message)\n",
    "        except:\n",
    "            return \"Error Query\", query\n",
    "        return res['result'], query\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2f68a4d7a54f02a07275289e0b64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = Neo4jGraph(url=\"neo4j+s://bed4a8a0.databases.neo4j.io\", username=\"neo4j\", password=\"XbfIA4FEeKePbNnhGT2YtqQ8zqPCcSR7XYYMQbbb70I\")\n",
    "file = \"Final Test Input.csv\"\n",
    "testTool = TPT(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [15:03<00:00,  9.04s/it]\n"
     ]
    }
   ],
   "source": [
    "testTool(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
